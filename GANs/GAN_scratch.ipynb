{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Development - Kochems Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from data_reader import DataReader\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import plotly.figure_factory as ff\n",
    "import kaleido\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "wandb_dir = os.path.expanduser(\"~\")\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ask_bid_int(dataset):\n",
    "    mask_ask = np.char.endswith(dataset[:,:,2], 'ask')\n",
    "    mask_bid = np.char.endswith(dataset[:,:,2], 'bid')\n",
    "    dataset[:,:,2][mask_ask] = '1'\n",
    "    dataset[:,:,2][mask_bid] = '0'\n",
    "    dataset = dataset.astype(np.float32)\n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "def get_dataset_max_price(dataset, rows_per_orderbook, level = -1):\n",
    "    index_on_each_OB = 0\n",
    "    if level != -1:\n",
    "        index_on_each_OB = (rows_per_orderbook//2-(level))\n",
    "    last_row_prices = dataset[:, index_on_each_OB, 0]\n",
    "    max_val = np.max(last_row_prices)\n",
    "    return max_val\n",
    "\n",
    "def get_dataset_min_price(dataset, rows_per_orderbook, level = -1):\n",
    "    index_on_each_OB = -1\n",
    "    if level != -1:\n",
    "        index_on_each_OB = (rows_per_orderbook//2+(level-1))\n",
    "    first_row_prices = dataset[:, index_on_each_OB, 0]\n",
    "    min_val = np.min(first_row_prices)\n",
    "    return min_val\n",
    "\n",
    "def make_histogram_from_dataset(dataset, rows_per_orderbook = 100, bin_width = 0.5, level = -1):\n",
    "    X_train = []\n",
    "    hist_max = get_dataset_max_price(dataset, rows_per_orderbook, level)\n",
    "    hist_min = get_dataset_min_price(dataset, rows_per_orderbook, level)\n",
    "    print(\"range: \", hist_min, \" \", hist_max)\n",
    "    num_bins = int(np.ceil((hist_max-hist_min) / bin_width))\n",
    "    bins = np.linspace(hist_min, hist_max, num_bins)\n",
    "    for i in range(len(dataset)):\n",
    "        orderbook = dataset[i]\n",
    "        price = orderbook[:,0]\n",
    "        quantity = orderbook[:,1]\n",
    "        quantity[orderbook[:, 2] == 0] *= -1\n",
    "        hist, bin_edges = np.histogram(price, bins=bins, weights=quantity)\n",
    "        X_train.append(hist)\n",
    "    X_train = np.array(X_train)\n",
    "    return X_train, hist_min, hist_max, bins\n",
    "\n",
    "def make_centred_LOB_snapshots(histograms, level = 1):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for i in range(len(histograms)-1):\n",
    "        current_OB = histograms[i]\n",
    "        next_OB = histograms[i+1]\n",
    "        j = -1\n",
    "        while j < len(current_OB)-1 and not (current_OB[j] < 0 and current_OB[j+1] > 0): j+=1\n",
    "        j+=1\n",
    "        \n",
    "        current_start_index = j-level\n",
    "        current_subarray_size = 2 * level\n",
    "        current_centre_LOB_snapshot = current_OB[current_start_index: current_start_index + current_subarray_size]\n",
    "        \n",
    "        next_subarray_size = 2 * current_subarray_size\n",
    "        next_centre_LOB_snapshot = next_OB[current_start_index - level: (current_start_index - level) + next_subarray_size]\n",
    "        \n",
    "        if np.any(next_centre_LOB_snapshot == 0):\n",
    "            continue\n",
    "        \n",
    "        X_train.append(current_centre_LOB_snapshot)\n",
    "        y_train.append(next_centre_LOB_snapshot)\n",
    "    \n",
    "    X_train = np.vstack(X_train)\n",
    "    y_train = np.vstack(y_train)\n",
    "    \n",
    "    return X_train, y_train\n",
    "        \n",
    "def normalization(X_train, c=8):\n",
    "    sqrt_abs_over_c = np.sqrt(np.abs(X_train)) / c\n",
    "    with_sign = np.sign(X_train) * sqrt_abs_over_c\n",
    "    return with_sign\n",
    "\n",
    "def get_centre_of_LOB(X_t_delta_t, level=3):\n",
    "    index = -1\n",
    "    for i in range(len(X_t_delta_t)-1):\n",
    "        if X_t_delta_t[i] < 0 and X_t_delta_t[i+1]>0: index = i+1\n",
    "    return X_t_delta_t[(index-level): (index-level) + 2 * level]\n",
    "\n",
    "def get_centre_of_LOB_dataset(X_t_delta_t_dataset, level = 3):\n",
    "    centres = []\n",
    "    for i in range(len(X_t_delta_t_dataset)):\n",
    "        if np.any(X_t_delta_t_dataset[i] == 0): continue\n",
    "        centre = get_centre_of_LOB(X_t_delta_t_dataset[i], level)\n",
    "        if centre.size != 6: continue\n",
    "        centres.append(centre)\n",
    "    np.array(centres)\n",
    "    return centres\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to post-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_normalization(X_train_normalized, c=8):\n",
    "    if not isinstance(X_train_normalized, np.ndarray) and X_train_normalized.device.type == 'cuda':\n",
    "        X_train_normalized = X_train_normalized.cpu()\n",
    "    unscaled_data = X_train_normalized * c\n",
    "    X_train = np.sign(X_train_normalized) * (unscaled_data ** 2)\n",
    "    return X_train\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_avg_centre_comparison(X_t_delta_t_generated_dataset, real_dataset, level = 3):\n",
    "    generated_dataset_centres = get_centre_of_LOB_dataset(X_t_delta_t_generated_dataset, level)\n",
    "    mean_real = np.mean(real_dataset, axis=0)\n",
    "    mean_fake = np.mean(generated_dataset_centres, axis=0)\n",
    "    interleaved_array = np.empty(mean_real.size + mean_fake.size, dtype=mean_real.dtype)\n",
    "    interleaved_array[0::2] = mean_real\n",
    "    interleaved_array[1::2] = mean_fake\n",
    "    indices = np.arange(len(interleaved_array))\n",
    "    return indices, interleaved_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actually reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = DataReader(\"./orderbook_snapshots.csv\", rows_per_orderbook=100)\n",
    "data_reader.read_csv()\n",
    "X_train_raw = data_reader.get_data()\n",
    "X_train_raw = convert_ask_bid_int(X_train_raw)\n",
    "print(X_train_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prices = (X_train_raw[:, 49, 0] + X_train_raw[:, 50, 0]) /2\n",
    "best_ask_quantities = X_train_raw[:, 49, 1]\n",
    "best_bid_quantities = np.abs(X_train_raw[:, 50, 1])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(best_prices, label='Prices')\n",
    "axes[1].plot(best_ask_quantities, label='Ask Quantities')\n",
    "axes[1].plot(best_bid_quantities, label='Bid Quantities')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histograms, price_min, price_max, bins = make_histogram_from_dataset(X_train_raw, rows_per_orderbook=100, bin_width=0.5, level=-1)\n",
    "print(histograms.shape)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Marginal Histogram of all Orderbook States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_not_processed, y_train_not_processed = make_centred_LOB_snapshots(histograms, level=3)\n",
    "print(X_train_not_processed.shape,\" \",y_train_not_processed.shape)\n",
    "print(X_train_not_processed[0])\n",
    "print(y_train_not_processed[0])\n",
    "all_orders = X_train_not_processed.ravel()\n",
    "all_orders_next = y_train_not_processed.ravel()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(all_orders, bins=100, density=True)\n",
    "axes[1].hist(all_orders_next, bins=100, density=True)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "S_t_dataset = normalization(X_train=X_train_not_processed,c=6)\n",
    "X_t_delta_t_dataset = normalization(X_train=y_train_not_processed,c=6)\n",
    "print(S_t_dataset.shape,\" \", X_t_delta_t_dataset.shape)\n",
    "print(S_t_dataset[0])\n",
    "print(X_t_delta_t_dataset[0])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(S_t_dataset.ravel(), bins=100, density=True)\n",
    "axes[1].hist(X_t_delta_t_dataset.ravel(), bins=100, density=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal Distributions (Normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Replace inf and -inf with NaN in your dataset columns\n",
    "X_t_delta_t_dataset = np.where(np.isinf(X_t_delta_t_dataset), np.nan, X_t_delta_t_dataset)\n",
    "\n",
    "# Assuming you've defined X_t_delta_t_dataset\n",
    "asks_1 = X_t_delta_t_dataset[:, 6].astype(float)\n",
    "asks_2 = X_t_delta_t_dataset[:, 7].astype(float)\n",
    "asks_3 = X_t_delta_t_dataset[:, 8].astype(float)\n",
    "\n",
    "# Remove or handle NaN values if necessary\n",
    "# For example, drop NaN values for plotting\n",
    "asks_1 = asks_1[~np.isnan(asks_1)]\n",
    "asks_2 = asks_2[~np.isnan(asks_2)]\n",
    "asks_3 = asks_3[~np.isnan(asks_3)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(asks_1, bins=100, density=True)\n",
    "sns.kdeplot(asks_1, ax=axes[0], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[1].hist(asks_2, bins=100, density=True)\n",
    "sns.kdeplot(asks_2, ax=axes[1], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[2].hist(asks_3, bins=100, density=True)\n",
    "sns.kdeplot(asks_3, ax=axes[2], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Repeat the process for bids\n",
    "bids_1 = X_t_delta_t_dataset[:, 5].astype(float)\n",
    "bids_2 = X_t_delta_t_dataset[:, 4].astype(float)\n",
    "bids_3 = X_t_delta_t_dataset[:, 3].astype(float)\n",
    "\n",
    "# Remove or handle NaN values if necessary\n",
    "bids_1 = bids_1[~np.isnan(bids_1)]\n",
    "bids_2 = bids_2[~np.isnan(bids_2)]\n",
    "bids_3 = bids_3[~np.isnan(bids_3)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(bids_1, bins=100, density=True)\n",
    "sns.kdeplot(bids_1, ax=axes[0], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[1].hist(bids_2, bins=100, density=True)\n",
    "sns.kdeplot(bids_2, ax=axes[1], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "axes[2].hist(bids_3, bins=100, density=True)\n",
    "sns.kdeplot(bids_3, ax=axes[2], color='red', fill=True, linewidth=0, alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices, interleaved_array = draw_avg_centre_comparison(X_t_delta_t_dataset, S_t_dataset, level = 3)\n",
    "plt.bar(indices, interleaved_array, color=['orange', 'blue'] * (len(interleaved_array) // 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_t_dataset_tensor = torch.tensor(S_t_dataset, dtype=torch.float32).to(device)\n",
    "X_t_delta_t_dataset_tensor = torch.tensor(X_t_delta_t_dataset, dtype=torch.float32).to(device)\n",
    "dataset = TensorDataset(X_t_delta_t_dataset_tensor, S_t_dataset_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# # First, select the relevant slices of the array for bid_1, bid_2, bid_3, ask_1, ask_2, and ask_3\n",
    "# data = X_t_delta_t_dataset[:, [6, 5, 7, 4, 8, 3]]\n",
    "\n",
    "# # Convert this data into a pandas DataFrame\n",
    "# columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "# df = pd.DataFrame(data, columns=columns)\n",
    "# df.dropna(inplace=True)     # Example: Drop rows with NaNs\n",
    "\n",
    "# # Compute the correlation matrix\n",
    "# correlation_matrix = df.corr()\n",
    "\n",
    "# # Plotting the correlation matrix as a heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, cbar=True, vmin=-1, vmax=1)\n",
    "# plt.title('Correlation Matrix of Order Book Levels')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# Assuming X_t_delta_t_dataset is correctly defined earlier in your script\n",
    "data = X_t_delta_t_dataset[:, [6, 5, 7, 4, 8, 3]]\n",
    "\n",
    "# Convert this data into a pandas DataFrame\n",
    "columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Ensure data types are numeric for correlation computation\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Generate the heatmap\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=correlation_matrix.to_numpy(),\n",
    "    x=correlation_matrix.columns.tolist(),\n",
    "    y=correlation_matrix.columns.tolist(),\n",
    "    annotation_text=correlation_matrix.round(2).to_numpy(),\n",
    "    colorscale='agsunset',\n",
    "    showscale=True,\n",
    "    zmin=-1, # Set minimum of scale\n",
    "    zmax=1  # Set maximum of scale\n",
    ")\n",
    "\n",
    "# Update layout to make it more readable\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix of Order Book Levels',\n",
    "    xaxis=dict(tickmode='array', tickvals=np.arange(len(columns)), ticktext=columns),\n",
    "    yaxis=dict(tickmode='array', tickvals=np.arange(len(columns)), ticktext=columns),\n",
    "    width=600, \n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, select the relevant slices of the array for bid_1, bid_2, bid_3, ask_1, ask_2, and ask_3\n",
    "data = X_t_delta_t_dataset[:, [6, 5, 7, 4, 8, 3]]\n",
    "\n",
    "# Convert this data into a pandas DataFrame\n",
    "columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WGAN-GP Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Critic and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=6, n=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.h_11 = nn.Linear(latent_dim,128).to(device)\n",
    "        self.h_12 = nn.Linear(n,128).to(device)\n",
    "        self.h_21 = nn.Linear(128,128).to(device)\n",
    "        self.h_22 = nn.Linear(128,128).to(device)\n",
    "        self.h_32 = nn.Linear(256, 256).to(device)\n",
    "        #output layer\n",
    "        self.output = nn.Linear(256, 12).to(device)\n",
    "        # self.h_35 = nn.Linear(6, 6).to(device)\n",
    "\n",
    "    def forward(self, Z_t, S_t):\n",
    "        Z_t = Z_t.to(device)  # Move input tensor to the device\n",
    "        S_t = S_t.to(device)  # Move input tensor to the device\n",
    "\n",
    "        h_11_output = torch.relu(self.h_11(Z_t))\n",
    "        h_21_output = torch.tanh(self.h_21(h_11_output))\n",
    "        \n",
    "        h_12_output = torch.relu(self.h_12(S_t))\n",
    "        h_22_output = torch.tanh(self.h_22(h_12_output))\n",
    "        \n",
    "        # Concatenation\n",
    "        h_31_output = torch.cat((h_21_output, h_22_output), dim=1)\n",
    "        \n",
    "        h_32_output = torch.tanh(self.h_32(h_31_output))\n",
    "        output = self.output(h_31_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "'''\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=6, n=6):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(latent_dim + n, 128).to(device),\n",
    "        nn.ReLU().to(device),\n",
    "        nn.Linear(128, 128).to(device),\n",
    "        nn.ReLU().to(device),\n",
    "        nn.Linear(128, 128).to(device),\n",
    "        nn.ReLU().to(device),\n",
    "        nn.Linear(128, 12).to(device)\n",
    "        )\n",
    "\n",
    "    def forward(self, Z_t, S_t):\n",
    "        Z_t = Z_t.to(device)\n",
    "        S_t = S_t.to(device)\n",
    "        combined_input = torch.cat((Z_t, S_t), dim = 1)\n",
    "        output = self.model(combined_input)\n",
    "        return output\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_1=12, n_2=6):\n",
    "        super(Critic, self).__init__()\n",
    "        # Increasing the depth and width of the network\n",
    "        self.model = nn.Sequential(\n",
    "        nn.Linear(n_1 + n_2, 128).to(device),\n",
    "        nn.ReLU().to(device),\n",
    "        nn.Linear(128, 128).to(device),\n",
    "        nn.ReLU().to(device),\n",
    "        nn.Linear(128, 128).to(device),\n",
    "        nn.ReLU().to(device),\n",
    "        nn.Linear(128, 1).to(device)\n",
    "        )\n",
    "\n",
    "    def forward(self, X_t_delta_t, S_t):\n",
    "        X_t_delta_t = X_t_delta_t.to(device)\n",
    "        S_t = S_t.to(device)\n",
    "        combined_input = torch.cat((X_t_delta_t, S_t), dim=1)\n",
    "        validity = self.model(combined_input)\n",
    "        return validity\n",
    "'''\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, n_1=12, n_2=6):\n",
    "        super(Critic, self).__init__()\n",
    "        self.h_11 = nn.Linear(n_1,128).to(device)\n",
    "        self.h_12 = nn.Linear(n_2,128).to(device)\n",
    "        self.h_21 = nn.Linear(128,128).to(device)\n",
    "        self.h_22 = nn.Linear(128,128).to(device)\n",
    "        \n",
    "        self.h_32 = nn.Linear(256, 256).to(device)\n",
    "        #output layer\n",
    "        self.output = nn.Linear(256, 1).to(device)\n",
    "\n",
    "    def forward(self, X_t_delta_t, S_t):\n",
    "        X_t_delta_t = X_t_delta_t.to(device)  # Move input tensor to the device\n",
    "        S_t = S_t.to(device)  # Move input tensor to the device\n",
    "\n",
    "        h_11_output = torch.relu(self.h_11(X_t_delta_t))\n",
    "        h_21_output = torch.relu(self.h_21(h_11_output))\n",
    "        \n",
    "        h_12_output = torch.relu(self.h_12(S_t))\n",
    "        h_22_output = torch.relu(self.h_22(h_12_output))\n",
    "\n",
    "        # Concatenation\n",
    "        h_31_output = torch.cat((h_21_output, h_22_output), dim=1)\n",
    "\n",
    "        h_32_output = torch.relu(self.h_32(h_31_output))\n",
    "        output = self.output(h_32_output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "n_critic = 5\n",
    "z_t_dim = 12\n",
    "LAMBDA = 10\n",
    "epochs = 100000\n",
    "learning_rate = 0.00001\n",
    "start_epoch = 0\n",
    "\n",
    "# Instantiate the generator and discriminator with device adaptation\n",
    "generator = Generator(latent_dim=z_t_dim, n=6).to(device)\n",
    "critic = Critic(n_1=12, n_2=6).to(device)\n",
    "\n",
    "# Define the optimisers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0, 0.9))\n",
    "optimizer_D = optim.RMSprop(critic.parameters(), lr=learning_rate, eps=1e-8, alpha=0.99)\n",
    "\n",
    "avg_d_loss_list = []\n",
    "avg_g_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wandb run location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runname = \"HorizontalExpansionBoth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE BELOW TO LOAD EXISTING RUN ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"uolse\",\n",
    "    id=\"s1ev1dzh\",\n",
    "    resume=\"must\",\n",
    "    dir=wandb_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USE BELOW TO CREATE EXISTING EXISTING RUN ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"uolse\",\n",
    "    name=runname,\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture generator\": Generator,\n",
    "    \"architecture critic\": Critic,\n",
    "    \"dataset\": \"OurDataset\",\n",
    "    \"epochs\": epochs,\n",
    "    \"z_t_dim\": z_t_dim,\n",
    "    \"LAMBDA\": LAMBDA,\n",
    "    \"runname\": runname,\n",
    "    },\n",
    "    dir=wandb_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wandb functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_marginal_distributions_to_wandb(epoch, X_t_delta_t_generated):\n",
    "    # Function to create and log a KDE plot\n",
    "    def log_kde_plot(data, title, epoch):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(data.cpu().numpy(), bins=100, density=True)\n",
    "        sns.kdeplot(data.cpu().numpy(), color='red', fill=True, alpha=0.5)\n",
    "        plt.title(title)\n",
    "        # Save the plot to a buffer\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        image = np.array(plt.imread(buf))\n",
    "        plt.close()\n",
    "        # Log the plot to Weights & Biases\n",
    "        wandb.log({f'KDE Plot {title}': wandb.Image(image, caption=title)}, step=epoch)\n",
    "    \n",
    "    # Indices for asks and bids might need adjustment based on dataset structure\n",
    "    asks_bids_indices = {\n",
    "        'Ask 1': 6,\n",
    "        'Ask 2': 7,\n",
    "        'Ask 3': 8,\n",
    "        'Bid 1': 5,\n",
    "        'Bid 2': 4,\n",
    "        'Bid 3': 3,\n",
    "    }\n",
    "\n",
    "    for title, index in asks_bids_indices.items():\n",
    "        data = X_t_delta_t_generated[:, index]\n",
    "        log_kde_plot(data, title, epoch)\n",
    "        \n",
    "def write_covariance_matrix_of_generated_data_to_wandb(epoch, X_t_delta_t_generated):\n",
    "    data = X_t_delta_t_generated[:, [6, 5, 7, 4, 8, 3]].cpu().numpy()\n",
    "    columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df = df.apply(pd.to_numeric, errors='coerce')\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    fig = ff.create_annotated_heatmap(\n",
    "        z=correlation_matrix.to_numpy(),\n",
    "        x=correlation_matrix.columns.tolist(),\n",
    "        y=correlation_matrix.columns.tolist(),\n",
    "        annotation_text=correlation_matrix.round(2).to_numpy(),\n",
    "        colorscale='agsunset',\n",
    "        showscale=True,\n",
    "        zmin=-1, \n",
    "        zmax=1\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='Correlation Matrix of Order Book Levels',\n",
    "        xaxis=dict(tickmode='array', tickvals=np.arange(len(columns)), ticktext=columns),\n",
    "        yaxis=dict(tickmode='array', tickvals=np.arange(len(columns)), ticktext=columns),\n",
    "        width=600, \n",
    "        height=600,\n",
    "    )\n",
    "\n",
    "    # Convert Plotly fig to PNG image using kaleido\n",
    "    img_bytes = fig.to_image(format=\"png\")\n",
    "    image = Image.open(io.BytesIO(img_bytes))\n",
    "    # Convert the PIL image to a NumPy array\n",
    "    image_array = np.array(image)\n",
    "    wandb.log({'Correlation Matrix of Order Book Levels': wandb.Image(image_array, caption='Correlation Matrix of Order Book Levels')}, step=epoch)\n",
    "    \n",
    "def write_pair_scatter_plot_to_wandb(epoch, X_t_delta_t_generated):\n",
    "    best_k_ask_bids = X_t_delta_t_generated[:, [6, 5, 7, 4, 8, 3]].cpu().numpy()\n",
    "    columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "    df = pd.DataFrame(best_k_ask_bids, columns=columns)\n",
    "    plt.figure()\n",
    "    sns.pairplot(df)\n",
    "    plt.title('Scatter plot of Orderbook Levels')\n",
    "    # Convert plot to image bytes\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    image = plt.imread(buf)\n",
    "    plt.close()\n",
    "    # Log the image to Weights & Biases\n",
    "    wandb.log({'Scatter plot of Orderbook Levels': wandb.Image(image, caption='Scatter plot of Orderbook Levels')}, step=epoch)\n",
    "\n",
    "def write_overlay_pair_scatter_plot_to_wandb(epoch, X_t_delta_t_generated, X_t_delta_t_dataset):\n",
    "    generated_data = X_t_delta_t_generated[:, [6, 5, 7, 4, 8, 3]].cpu().numpy()\n",
    "    training_data = X_t_delta_t_dataset[:, [6, 5, 7, 4, 8, 3]]\n",
    "\n",
    "    columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "    df_generated = pd.DataFrame(generated_data, columns=columns)\n",
    "    df_training = pd.DataFrame(training_data, columns=columns)\n",
    "\n",
    "    # Overlay DataFrames with key\n",
    "    df_combined = pd.concat([df_generated.assign(dataset='Generated'), df_training.assign(dataset='Training')])\n",
    "\n",
    "    plt.figure()\n",
    "    sns.pairplot(df_combined, hue='dataset', palette='bright', diag_kind='kde', plot_kws={'alpha':0.6})\n",
    "    plt.suptitle('Overlay Scatter plot of Orderbook Levels', y=1.02)  # Adjust title placement\n",
    "\n",
    "    # Convert plot to image bytes\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight') \n",
    "    buf.seek(0)\n",
    "    image = plt.imread(buf)\n",
    "    plt.close()\n",
    "\n",
    "    # Log the image to Weights & Biases\n",
    "    wandb.log({'Overlay Scatter plot of Orderbook Levels': wandb.Image(image, caption='Overlay Scatter plot of Orderbook Levels')}, step=epoch)\n",
    "    \n",
    "def write_metrics_to_wandb(epoch):\n",
    "    with torch.no_grad():\n",
    "        z_t = torch.randn(S_t_dataset.shape[0], z_t_dim)\n",
    "        X_t_delta_t_generated = generator(z_t, S_t_dataset_tensor)\n",
    "    write_marginal_distributions_to_wandb(epoch, X_t_delta_t_generated)\n",
    "    write_covariance_matrix_of_generated_data_to_wandb(epoch, X_t_delta_t_generated)\n",
    "    # write_pair_scatter_plot_to_wandb(epoch, X_t_delta_t_generated)\n",
    "    write_overlay_pair_scatter_plot_to_wandb(epoch, X_t_delta_t_generated, X_t_delta_t_dataset)\n",
    "\n",
    "        \n",
    "def write_loss_functions_to_wandb(avg_d_loss, avg_g_loss,epoch):\n",
    "    wandb.log({'Loss/Discriminator': avg_d_loss, 'Loss/Generator': avg_g_loss}, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to save and load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, generator, discriminator, optimizer_G, optimizer_D, avg_d_loss, avg_g_loss, filepath):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'generator_state_dict': generator.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "        'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        'avg_d_loss': avg_d_loss,\n",
    "        'avg_g_loss': avg_g_loss\n",
    "    }, filepath)\n",
    "\n",
    "\n",
    "def load_checkpoint(filepath, generator, discriminator, optimizer_G, optimizer_D):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
    "    optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    avg_d_loss = checkpoint['avg_d_loss']\n",
    "    avg_g_loss = checkpoint['avg_g_loss']\n",
    "    return epoch, avg_d_loss, avg_g_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONLY RUN IF YOU WANT TO LOAD A MODEL AND CONTINUE IT FOR A RUN. MAKE SURE U LOAD WANDB RUN AND NOT CREATE A NEW WANDB RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch, avg_d_loss, avg_g_loss = load_checkpoint('./saved_models/python/HorizontalExpansionBoth.pth', generator, critic, optimizer_G, optimizer_D)\n",
    "print(f\"Resuming training from epoch {start_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(C, real_samples, fake_samples, batch_S_t):\n",
    "    batch_size = real_samples.size(0)\n",
    "    # Ensure alpha is shaped correctly for broadcasting\n",
    "    alpha = torch.rand(batch_size, 1, device=device)\n",
    "    alpha = alpha.expand(batch_size, real_samples.nelement() // batch_size).contiguous().view(batch_size, -1)\n",
    "\n",
    "    # Calculate interpolates\n",
    "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
    "    d_interpolates = C(interpolates, batch_S_t)\n",
    "    \n",
    "    fake = Variable(torch.Tensor(batch_size , 1).fill_(1.0), requires_grad=False).to(device)\n",
    "    gradients = grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=fake,\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    # Flatten the gradients\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop of WGAN with Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables to track progress\n",
    "avg_d_loss = 0\n",
    "avg_g_loss = 0\n",
    "n_batches = len(data_loader)\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    total_d_loss_epoch = 0\n",
    "    total_g_loss_epoch = 0\n",
    "    \n",
    "    for i, (batch_X_t_delta_t, batch_S_t) in enumerate(data_loader):\n",
    "        batch_size = batch_S_t.shape[0]\n",
    "        \n",
    "        batch_S_t = batch_S_t.to(device)\n",
    "        batch_X_t_delta_t = batch_X_t_delta_t.to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Critic\n",
    "        # ---------------------\n",
    "        total_d_loss_batch = 0\n",
    "        for _ in range(n_critic):\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            batch_Z_t = torch.randn(batch_size, z_t_dim, device=device)  # Ensure noise_dim matches generator input\n",
    "            batch_X_t_delta_t_generated = generator(batch_Z_t, batch_S_t)\n",
    "\n",
    "            real_validity = critic(batch_X_t_delta_t, batch_S_t)\n",
    "            fake_validity = critic(batch_X_t_delta_t_generated, batch_S_t)\n",
    "            gp = gradient_penalty(critic, batch_X_t_delta_t.data, batch_X_t_delta_t_generated.data, batch_S_t.data)\n",
    "            \n",
    "            # Wasserstein GAN loss w/ gradient penalty\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + LAMBDA * gp\n",
    "\n",
    "            d_loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_value_(critic.parameters(), 1.0)\n",
    "            optimizer_D.step()\n",
    "            total_d_loss_batch += d_loss.item()\n",
    "            \n",
    "        d_loss_batch = total_d_loss_batch / n_critic\n",
    "        total_d_loss_epoch += d_loss_batch # Average over the n_critic updates\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Resample noise (optional but often leads to better training stability)\n",
    "        batch_Z_t = torch.randn(batch_size, z_t_dim, device=device)\n",
    "        batch_X_t_delta_t_generated = generator(batch_Z_t, batch_S_t)\n",
    "        fake_validity = critic(batch_X_t_delta_t_generated, batch_S_t)\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        total_g_loss_epoch += g_loss.item()\n",
    "\n",
    "    # Prints average loss per epoch\n",
    "    avg_d_loss = total_d_loss_epoch / n_batches\n",
    "    avg_g_loss = total_g_loss_epoch / n_batches\n",
    "    avg_d_loss_list.append(avg_d_loss)\n",
    "    avg_g_loss_list.append(avg_g_loss)\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] completed. Avg D Loss: {avg_d_loss:.4f}, Avg G Loss: {avg_g_loss:.4f}\")\n",
    "        \n",
    "    if (epoch + 1) % 10000 == 0:\n",
    "        write_metrics_to_wandb(epoch)\n",
    "        \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        save_checkpoint(epoch, generator, critic, optimizer_G, optimizer_D, avg_d_loss, avg_g_loss, './saved_models/python/HorizontalExpansionBoth.pth')\n",
    "        \n",
    "        \n",
    "    write_loss_functions_to_wandb(avg_d_loss, avg_g_loss,epoch)\n",
    "    # Resets average losses for the next epoch\n",
    "    avg_d_loss = 0\n",
    "    avg_g_loss = 0\n",
    "\n",
    "# Close the writer after the training loop\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Save Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(epoch, generator, critic, optimizer_G, optimizer_D, avg_d_loss, avg_g_loss, './saved_models/python/HorizontalExpansion.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the generator's state dictionary for python\n",
    "# torch.save(generator.state_dict(), './saved_models/python/BatchSize64Base1000Epochs.pth')\n",
    "# # Save the generator's state dictionary for c++\n",
    "# z_t = torch.randn(1,z_t_dim)\n",
    "# s_t = torch.tensor(S_t_dataset[0], dtype=torch.float32).unsqueeze(0)\n",
    "# traced_script_generator = torch.jit.trace(generator, (z_t, s_t))\n",
    "# traced_script_generator.save('./saved_models/cpp/BatchSize64Base1000Epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    z_t = torch.randn(S_t_dataset.shape[0], z_t_dim)\n",
    "    X_t_delta_t_generated = generator(z_t, S_t_dataset_tensor)\n",
    "\n",
    "selu = nn.SELU()\n",
    "data = X_t_delta_t_generated\n",
    "data[:, 5] = data[:, 5]\n",
    "data[:, 6] = data[:, 6]\n",
    "condition_range = (-0.05, 0.05)\n",
    "# # Check condition for the 5th column\n",
    "condition_5th = (data[:, 5] >= condition_range[0]) & (data[:, 5] <= condition_range[1])\n",
    "condition_6th = (data[:, 6] >= condition_range[0]) & (data[:, 6] <= condition_range[1])\n",
    "\n",
    "# # Multiply values in the 5th column by 2 where condition is met\n",
    "# data[:, 5] = torch.where(condition_5th, 5*torch.tanh(data[:, 5]), data[:, 5])\n",
    "# data[:, 6] = torch.where(condition_6th, 5*torch.tanh(data[:, 6]), data[:, 6])\n",
    "\n",
    "X_t_delta_t_generated = data\n",
    "print(X_t_delta_t_generated.shape)\n",
    "best_k_ask_bids = X_t_delta_t_generated[:, [6, 5, 7, 4, 8, 3]].cpu().numpy()\n",
    "columns = ['Ask1', 'Bid1', 'Ask2', 'Bid2', 'Ask3', 'Bid3']\n",
    "df = pd.DataFrame(best_k_ask_bids, columns=columns)\n",
    "plt.figure()\n",
    "sns.pairplot(df)\n",
    "plt.title('Scatter plot of Orderbook Levels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 1 Example Orderbook Snapshot where the price has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample order book snapshot\n",
    "i=0\n",
    "counter = 0\n",
    "while (True):\n",
    "    with torch.no_grad():\n",
    "        if counter == 1: break\n",
    "        i += 1\n",
    "        z_t = torch.randn(1,z_t_dim)\n",
    "        s_t = torch.tensor(S_t_dataset[0], dtype=torch.float32).unsqueeze(0)\n",
    "        X_t_delta_t_example = generator(z_t, s_t)\n",
    "        if reverse_normalization(X_t_delta_t_example)[0, 5] > 0 or reverse_normalization(X_t_delta_t_example)[0, 6] < 0:\n",
    "            print(i)\n",
    "            print(\"Current Order Book Snapshot:\\t\", reverse_normalization(s_t))\n",
    "            print(\"Generated Order Book Snapshot:\\t\", reverse_normalization(X_t_delta_t_example))\n",
    "            counter += 1\n",
    "            \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ead1b95f633dc9c51826328e1846203f51a198c6fb5f2884a80417ba131d4e82"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
